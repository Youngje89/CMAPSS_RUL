{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 3089.562601).  Saving model ...\n",
      "Validation loss decreased (3089.562601 --> 1752.533138).  Saving model ...\n",
      "Validation loss decreased (1752.533138 --> 1515.931825).  Saving model ...\n",
      "Validation loss decreased (1515.931825 --> 968.565213).  Saving model ...\n",
      "Epoch [5/35], Train Loss: 606.9663, Val Loss: 474.2458\n",
      "Validation loss decreased (968.565213 --> 474.245758).  Saving model ...\n",
      "Validation loss decreased (474.245758 --> 342.747177).  Saving model ...\n",
      "Validation loss decreased (342.747177 --> 306.394882).  Saving model ...\n",
      "Validation loss decreased (306.394882 --> 275.240949).  Saving model ...\n",
      "Validation loss decreased (275.240949 --> 267.993475).  Saving model ...\n",
      "Epoch [10/35], Train Loss: 211.3746, Val Loss: 259.1301\n",
      "Validation loss decreased (267.993475 --> 259.130126).  Saving model ...\n",
      "Validation loss decreased (259.130126 --> 251.435049).  Saving model ...\n",
      "Validation loss decreased (251.435049 --> 249.243272).  Saving model ...\n",
      "Validation loss decreased (249.243272 --> 247.567189).  Saving model ...\n",
      "Epoch [15/35], Train Loss: 190.6146, Val Loss: 242.6634\n",
      "Validation loss decreased (247.567189 --> 242.663382).  Saving model ...\n",
      "Validation loss decreased (242.663382 --> 238.290875).  Saving model ...\n",
      "Validation loss decreased (238.290875 --> 238.114026).  Saving model ...\n",
      "Validation loss decreased (238.114026 --> 234.506723).  Saving model ...\n",
      "Epoch [20/35], Train Loss: 179.0479, Val Loss: 235.5024\n",
      "Validation loss decreased (234.506723 --> 234.254171).  Saving model ...\n",
      "Validation loss decreased (234.254171 --> 232.439552).  Saving model ...\n",
      "Epoch [25/35], Train Loss: 170.7037, Val Loss: 232.5607\n",
      "Validation loss decreased (232.439552 --> 230.154609).  Saving model ...\n",
      "Validation loss decreased (230.154609 --> 229.449173).  Saving model ...\n",
      "Epoch [30/35], Train Loss: 163.5384, Val Loss: 246.2928\n",
      "Validation loss decreased (229.449173 --> 224.819553).  Saving model ...\n",
      "Epoch [35/35], Train Loss: 158.1870, Val Loss: 225.3395\n",
      "Best model weights saved to MixStyle_30_1.pth\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from utils import EarlyStopping\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "dataset_names = ['FD002', 'FD004']\n",
    "\n",
    "concatname = ''\n",
    "for i in dataset_names:\n",
    "    concatname += (i[4])\n",
    "\n",
    "sensors = ['s_2', 's_3', 's_4', 's_7', 's_8', 's_9', 's_11', 's_12', 's_13', 's_14', 's_15', 's_17', 's_20', 's_21']\n",
    "alpha = 0.1\n",
    "threshold = 125\n",
    "window_size = 30\n",
    "sensor_size = len(sensors)\n",
    "forecast_size = 1\n",
    "batch_size = 64\n",
    "num_layers = 1\n",
    "beta = 1\n",
    "learning_rate = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 데이터셋과 데이터 로더를 저장할 딕셔너리\n",
    "train_domains_loader = {}\n",
    "val_domains_loader = {}\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = utils.get_data(dataset_name, sensors, window_size, alpha, threshold)\n",
    "    \n",
    "    # 데이터를 텐서로 변환하고 디바이스로 이동\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    # 딕셔너리에 저장\n",
    "    train_domains_loader[dataset_name] = {\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "    }\n",
    "    \n",
    "    val_domains_loader[dataset_name] = {\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "    }\n",
    "\n",
    "def calculate_mean_and_variance_per_sample(X):\n",
    "    means = np.mean(X, axis=1)  # axis=1, 각 샘플 별로 평균 계산\n",
    "    vars = np.var(X, axis=1)  # axis=1, 각 샘플 별로 분산 계산\n",
    "    return means, vars\n",
    "\n",
    "def mix_style_per_sample(X1, y1, X2, y2, lambda_beta):\n",
    "    min_len = min(len(X1), len(X2))\n",
    "    \n",
    "    # 각 샘플에 대한 평균과 분산 계산\n",
    "    means1, vars1 = calculate_mean_and_variance_per_sample(X1)\n",
    "    means2, vars2 = calculate_mean_and_variance_per_sample(X2)\n",
    "    \n",
    "    # 샘플링할 데이터의 인덱스 결정\n",
    "    indices1 = np.random.permutation(len(X1))[:min_len]\n",
    "    indices2 = np.random.permutation(len(X2))[:min_len]\n",
    "    \n",
    "    # 혼합 비율 샘플링\n",
    "    lambda_vals = np.random.beta(lambda_beta, lambda_beta, size=min_len)\n",
    "    \n",
    "    # MixStyle 적용\n",
    "    X_mixed = np.empty_like(X1[:min_len])\n",
    "    for i in range(min_len):\n",
    "        idx1 = indices1[i]\n",
    "        idx2 = indices2[i]\n",
    "        lambda_val = lambda_vals[i]\n",
    "        X_mixed[i] = lambda_val * (X1[idx1] - means1[idx1]) / np.sqrt(vars1[idx1]) + \\\n",
    "                     (1 - lambda_val) * (X2[idx2] - means2[idx2]) / np.sqrt(vars2[idx2])\n",
    "        X_mixed[i] = X_mixed[i] * np.sqrt(lambda_val * vars1[idx1] + (1 - lambda_val) * vars2[idx2]) + \\\n",
    "                     lambda_val * means1[idx1] + (1 - lambda_val) * means2[idx2]      \n",
    "    \n",
    "    # y의 혼합을 위해 브로드캐스팅 가능한 형태로 lambda_vals 조정\n",
    "    lambda_vals_reshaped = lambda_vals[:, None]\n",
    "    \n",
    "    # y 혼합\n",
    "    y_mixed = lambda_vals_reshaped * y1[indices1] + (1 - lambda_vals_reshaped) * y2[indices2]     \n",
    "    \n",
    "    return X_mixed, y_mixed\n",
    "\n",
    "# 도메인1, 2 각각의 X_train과 y_train 혼합\n",
    "lambda_beta = 0.2\n",
    "train_X_mixed, train_y_mixed = mix_style_per_sample(train_domains_loader[dataset_names[0]][\"X_train\"], train_domains_loader[dataset_names[0]][\"y_train\"],\n",
    "                             train_domains_loader[dataset_names[1]][\"X_train\"], train_domains_loader[dataset_names[1]][\"y_train\"],\n",
    "                             lambda_beta)\n",
    "val_X_mixed, val_y_mixed = mix_style_per_sample(val_domains_loader[dataset_names[0]][\"X_val\"], val_domains_loader[dataset_names[0]][\"y_val\"],\n",
    "                             val_domains_loader[dataset_names[1]][\"X_val\"], val_domains_loader[dataset_names[1]][\"y_val\"],\n",
    "                             lambda_beta)\n",
    "\n",
    "# 혼합된 데이터를 텐서로 변환\n",
    "train_X_mixed_tensor = torch.tensor(train_X_mixed, dtype=torch.float32)\n",
    "train_y_mixed_tensor = torch.tensor(train_y_mixed, dtype=torch.float32)\n",
    "val_X_mixed_tensor = torch.tensor(val_X_mixed, dtype=torch.float32)\n",
    "val_y_mixed_tensor = torch.tensor(val_y_mixed, dtype=torch.float32)\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "# Define the GRU model\n",
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, drop_prob=0.2):\n",
    "        super(BiGRU, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True, dropout=drop_prob, bidirectional=True)\n",
    "        self.fc = nn.Linear(2*hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        gru_out = gru_out[:, -1, :]\n",
    "        output = self.fc(gru_out)\n",
    "        return output\n",
    "\n",
    "# Function to extract features using CNN\n",
    "def extract_features(cnn_model, data):\n",
    "    cnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = cnn_model(data.permute(0, 2, 1).to(device))\n",
    "        features = features.permute(0, 2, 1)\n",
    "    return features\n",
    "\n",
    "# Function to create DataLoader\n",
    "def create_dataloader(X, y, batch_size):\n",
    "    dataset = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "# Create CNN model and move to device\n",
    "cnn_model = CNNFeatureExtractor(input_channels=sensor_size).to(device)\n",
    "\n",
    "# Extract features using CNN\n",
    "train_X_features = extract_features(cnn_model, train_X_mixed_tensor)\n",
    "val_X_features = extract_features(cnn_model, val_X_mixed_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "mixed_train_loader = create_dataloader(train_X_features, train_y_mixed_tensor, batch_size)\n",
    "mixed_val_loader = create_dataloader(val_X_features, val_y_mixed_tensor, batch_size)\n",
    "\n",
    "# Create GRU model and move to device\n",
    "hidden_dim = 32\n",
    "output_dim = 1\n",
    "rnn_model = BiGRU(input_dim=train_X_features.shape[2], hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop with early stopping\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "early_stopping = EarlyStopping(patience=35, verbose=True, delta=0)\n",
    "epochs = 35\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    rnn_model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(mixed_train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = rnn_model(inputs)\n",
    "        train_loss = criterion(outputs, targets)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += train_loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(mixed_train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    rnn_model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in mixed_val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = rnn_model(inputs)\n",
    "            val_loss = criterion(outputs, targets)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(mixed_val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    early_stopping(avg_val_loss, rnn_model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "save_path = f\"MixStyle_{window_size}_{forecast_size}.pth\"\n",
    "torch.save(rnn_model.state_dict(), save_path)\n",
    "print(f\"Best model weights saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD001 RMSE 57.56209945678711 R2 -1.0633058547973633 score [993221.44]\n",
      "FD002 RMSE 53.70090866088867 R2 -0.5640617609024048 score [535361.9]\n",
      "FD003 RMSE 36.491580963134766 R2 0.13190943002700806 score [38099.766]\n",
      "FD004 RMSE 53.88380432128906 R2 -0.5716667175292969 score [683330.5]\n"
     ]
    }
   ],
   "source": [
    "utils.evaluateDataset_1DCNN(\"FD001\", rnn_model)\n",
    "utils.evaluateDataset_1DCNN(\"FD002\", rnn_model)\n",
    "utils.evaluateDataset_1DCNN(\"FD003\", rnn_model)\n",
    "utils.evaluateDataset_1DCNN(\"FD004\", rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [100, 43523]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m RMSE \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m R2 \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m score \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dataset, rmse, variance, score))\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Evaluate the model's performance on the test set\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_actual_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 41\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(y_true, y_hat, label)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(y_true, y_hat, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 41\u001b[0m     mse \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mse)\n\u001b[0;32m     43\u001b[0m     variance \u001b[38;5;241m=\u001b[39m r2_score(y_true, y_hat)\n",
      "File \u001b[1;32mc:\\Users\\Youngje Oh\\.conda\\envs\\RUL\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Youngje Oh\\.conda\\envs\\RUL\\lib\\site-packages\\sklearn\\metrics\\_regression.py:506\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[0;32m    503\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[0;32m    504\u001b[0m         )\n\u001b[1;32m--> 506\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    510\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\Youngje Oh\\.conda\\envs\\RUL\\lib\\site-packages\\sklearn\\metrics\\_regression.py:111\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, multioutput, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m--> 111\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\Youngje Oh\\.conda\\envs\\RUL\\lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [100, 43523]"
     ]
    }
   ],
   "source": [
    "dataset = \"FD001\"\n",
    "\n",
    "# sensors to work with: 14 sensors\n",
    "#sensors = ['s_3', 's_4', 's_7', 's_11', 's_12']\n",
    "sensors = ['s_2','s_3','s_4','s_7','s_8','s_9','s_11','s_12','s_13','s_14','s_15','s_17','s_20','s_21']\n",
    "window_size = 30\n",
    "alpha = 0.1\n",
    "threshold = 125\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = utils.get_data(dataset, sensors, window_size, alpha, threshold)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "    \n",
    "train_X_features = extract_features(cnn_model, X_train_tensor)\n",
    "# Create DataLoaders\n",
    "mixed_train_loader = create_dataloader(train_X_features, y_train_tensor, batch_size)\n",
    "\n",
    "# Ensure the model is in evaluation mode and move it to the correct device\n",
    "rnn_model.eval()\n",
    "rnn_model.to(device)\n",
    "\n",
    "# Use the model to predict y_test\n",
    "with torch.no_grad():\n",
    "    #model.load_state_dict(torch.load('C:/Users/User/exploring-nasas-turbofan-dataset/weights/linear_model_weights_20_5.pth'))\n",
    "    predictions_tensor = rnn_model(train_X_features)\n",
    "    \n",
    "    # Clamp the predictions to be within [0, 125]\n",
    "    predictions_tensor = torch.clamp(predictions_tensor, 0, 125)\n",
    "    \n",
    "    predictions = predictions_tensor.cpu().numpy()\n",
    "\n",
    "# Convert y_test_tensor to numpy array\n",
    "y_actual_test = y_test_tensor.cpu().numpy()\n",
    "\n",
    "def evaluate(y_true, y_hat, label='test'):\n",
    "    mse = mean_squared_error(y_true, y_hat)\n",
    "    rmse = np.sqrt(mse)\n",
    "    variance = r2_score(y_true, y_hat)\n",
    "    \n",
    "    score = 0\n",
    "    #y_true = y_true.cpu()\n",
    "    #y_hat = y_hat.cpu()\n",
    "    for i in range(len(y_hat)):\n",
    "        if y_true[i] <= y_hat[i]:\n",
    "            score = score + np.exp(-(y_true[i] - y_hat[i]) / 10.0) - 1\n",
    "        else:\n",
    "            score = score + np.exp((y_true[i] - y_hat[i]) / 13.0) - 1\n",
    "    \n",
    "    print('{} RMSE {} R2 {} score {}'.format(dataset, rmse, variance, score))\n",
    "\n",
    "# Evaluate the model's performance on the test set\n",
    "evaluate(y_actual_test, predictions, label='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD001 RMSE 13.223660469055176 R2 0.8911085724830627 score [259.93713]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from utils import EarlyStopping\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "# Dataset and sensor settings\n",
    "dataset = \"FD001\"\n",
    "sensors = ['s_2','s_3','s_4','s_7','s_8','s_9','s_11','s_12','s_13','s_14','s_15','s_17','s_20','s_21']\n",
    "window_size = 30\n",
    "alpha = 0.1\n",
    "threshold = 125\n",
    "\n",
    "# Load data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = utils.get_data(dataset, sensors, window_size, alpha, threshold)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convert data to tensors and move to device\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "# Ensure the model is in evaluation mode and move it to the correct device\n",
    "cnn_model.eval()\n",
    "rnn_model.eval()\n",
    "cnn_model.to(device)\n",
    "rnn_model.to(device)\n",
    "\n",
    "# Extract features using CNN for test set\n",
    "test_X_features = extract_features(cnn_model, X_test_tensor)\n",
    "\n",
    "# Use the model to predict y_test\n",
    "with torch.no_grad():\n",
    "    predictions_tensor = rnn_model(test_X_features)\n",
    "    \n",
    "    # Clamp the predictions to be within [0, 125]\n",
    "    predictions_tensor = torch.clamp(predictions_tensor, 0, 125)\n",
    "    \n",
    "    predictions = predictions_tensor.cpu().numpy()\n",
    "\n",
    "# Convert y_test_tensor to numpy array\n",
    "y_actual_test = y_test_tensor.cpu().numpy()\n",
    "\n",
    "def evaluate(y_true, y_hat, label='test'):\n",
    "    mse = mean_squared_error(y_true, y_hat)\n",
    "    rmse = np.sqrt(mse)\n",
    "    variance = r2_score(y_true, y_hat)\n",
    "    \n",
    "    score = 0\n",
    "    for i in range(len(y_hat)):\n",
    "        if y_true[i] <= y_hat[i]:\n",
    "            score = score + np.exp(-(y_true[i] - y_hat[i]) / 10.0) - 1\n",
    "        else:\n",
    "            score = score + np.exp((y_true[i] - y_hat[i]) / 13.0) - 1\n",
    "    \n",
    "    print('{} RMSE {} R2 {} score {}'.format(dataset, rmse, variance, score))\n",
    "\n",
    "# Evaluate the model's performance on the test set\n",
    "evaluate(y_actual_test, predictions, label='test')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RUL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
